{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a53c8d-11d2-4017-bc25-1e65e5e40f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"OlympicDataset CSV Loader\").getOrCreate()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef5d4b3-17ab-4ef5-9162-8ebd8ca2db0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/FileStore/tables/Athletes-1.csv', name='Athletes-1.csv', size=418492, modificationTime=1734225612000),\n FileInfo(path='dbfs:/FileStore/tables/Athletes.csv', name='Athletes.csv', size=418492, modificationTime=1734225047000),\n FileInfo(path='dbfs:/FileStore/tables/Coaches-1.csv', name='Coaches-1.csv', size=16889, modificationTime=1734225612000),\n FileInfo(path='dbfs:/FileStore/tables/Coaches.csv', name='Coaches.csv', size=16889, modificationTime=1734225047000),\n FileInfo(path='dbfs:/FileStore/tables/EntriesGender-1.csv', name='EntriesGender-1.csv', size=1123, modificationTime=1734225612000),\n FileInfo(path='dbfs:/FileStore/tables/EntriesGender.csv', name='EntriesGender.csv', size=1123, modificationTime=1734225047000),\n FileInfo(path='dbfs:/FileStore/tables/Medals-1.csv', name='Medals-1.csv', size=2414, modificationTime=1734225612000),\n FileInfo(path='dbfs:/FileStore/tables/Medals.csv', name='Medals.csv', size=2414, modificationTime=1734225047000),\n FileInfo(path='dbfs:/FileStore/tables/Medals_rename.csv/', name='Medals_rename.csv/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/Teams-1.csv', name='Teams-1.csv', size=35270, modificationTime=1734225612000),\n FileInfo(path='dbfs:/FileStore/tables/Teams.csv', name='Teams.csv', size=35270, modificationTime=1734225048000),\n FileInfo(path='dbfs:/FileStore/tables/bronze_data/', name='bronze_data/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/large_sample_data.csv', name='large_sample_data.csv', size=56041, modificationTime=1725661495000),\n FileInfo(path='dbfs:/FileStore/tables/sample_data-1.csv', name='sample_data-1.csv', size=128, modificationTime=1725661344000),\n FileInfo(path='dbfs:/FileStore/tables/sample_data.csv', name='sample_data.csv', size=128, modificationTime=1725661258000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e544a113-8b4a-4aa8-9c9e-c1a4fd571b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "Athletesdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Athletes.csv\")\n",
    "Coachesdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Coaches.csv\")\n",
    "Entriesgenderdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/EntriesGender.csv\")\n",
    "Medalsdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Medals.csv\")\n",
    "Teamsdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df54e9f3-67ec-47b2-b037-750ee75d7338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- PersonName: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Discipline: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Athletesdf.show()\n",
    "Athletesdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25bb61d1-c568-456b-a12a-c430c3520fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+--------+\n|                Name|             Country|       Discipline|   Event|\n+--------------------+--------------------+-----------------+--------+\n|     ABDELMAGID Wael|               Egypt|         Football|    null|\n|           ABE Junya|               Japan|       Volleyball|    null|\n|       ABE Katsuhiko|               Japan|       Basketball|    null|\n|        ADAMA Cherif|       Cï¿½te d'Ivoire|         Football|    null|\n|          AGEBA Yuya|               Japan|       Volleyball|    null|\n|AIKMAN Siegfried ...|               Japan|           Hockey|     Men|\n|       AL SAADI Kais|             Germany|           Hockey|     Men|\n|       ALAMEDA Lonni|              Canada|Baseball/Softball|Softball|\n|     ALEKNO Vladimir|Islamic Republic ...|       Volleyball|     Men|\n|     ALEKSEEV Alexey|                 ROC|         Handball|   Women|\n|ALLER CARBALLO Ma...|               Spain|       Basketball|    null|\n|       ALSHEHRI Saad|        Saudi Arabia|         Football|     Men|\n|           ALY Kamal|               Egypt|         Football|    null|\n| AMAYA GAITAN Fabian|         Puerto Rico|       Basketball|    null|\n|    AMO AGUADO Pablo|               Spain|         Football|    null|\n|   ANDONOVSKI Vlatko|United States of ...|         Football|   Women|\n|        ANNAN Alyson|         Netherlands|           Hockey|   Women|\n|  ARNAU CREUS Xavier|               Japan|           Hockey|   Women|\n|       ARNOLD Graham|           Australia|         Football|     Men|\n|         AXNER Tomas|              Sweden|         Handball|   Women|\n+--------------------+--------------------+-----------------+--------+\nonly showing top 20 rows\n\nroot\n |-- Name: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Discipline: string (nullable = true)\n |-- Event: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "Coachesdf.show()\n",
    "Coachesdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ce6f1c-774c-44e0-8f8a-77e3a9d050e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+-----+\n|          Discipline|Female|Male|Total|\n+--------------------+------+----+-----+\n|      3x3 Basketball|    32|  32|   64|\n|             Archery|    64|  64|  128|\n| Artistic Gymnastics|    98|  98|  196|\n|   Artistic Swimming|   105|   0|  105|\n|           Athletics|   969|1072| 2041|\n|           Badminton|    86|  87|  173|\n|   Baseball/Softball|    90| 144|  234|\n|          Basketball|   144| 144|  288|\n|    Beach Volleyball|    48|  48|   96|\n|              Boxing|   102| 187|  289|\n|        Canoe Slalom|    41|  41|   82|\n|        Canoe Sprint|   123| 126|  249|\n|Cycling BMX Frees...|    10|   9|   19|\n|  Cycling BMX Racing|    24|  24|   48|\n|Cycling Mountain ...|    38|  38|   76|\n|        Cycling Road|    70| 131|  201|\n|       Cycling Track|    90|  99|  189|\n|              Diving|    72|  71|  143|\n|          Equestrian|    73| 125|  198|\n|             Fencing|   107| 108|  215|\n+--------------------+------+----+-----+\nonly showing top 20 rows\n\nroot\n |-- Discipline: string (nullable = true)\n |-- Female: integer (nullable = true)\n |-- Male: integer (nullable = true)\n |-- Total: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "Entriesgenderdf.show()\n",
    "Entriesgenderdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53de7367-0180-4828-8164-2594b23caa7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----+------+------+-----+-------------+\n|Rank|         TeamCountry|Gold|Silver|Bronze|Total|Rank by Total|\n+----+--------------------+----+------+------+-----+-------------+\n|   1|United States of ...|  39|    41|    33|  113|            1|\n|   2|People's Republic...|  38|    32|    18|   88|            2|\n|   3|               Japan|  27|    14|    17|   58|            5|\n|   4|       Great Britain|  22|    21|    22|   65|            4|\n|   5|                 ROC|  20|    28|    23|   71|            3|\n|   6|           Australia|  17|     7|    22|   46|            6|\n|   7|         Netherlands|  10|    12|    14|   36|            9|\n|   8|              France|  10|    12|    11|   33|           10|\n|   9|             Germany|  10|    11|    16|   37|            8|\n|  10|               Italy|  10|    10|    20|   40|            7|\n|  11|              Canada|   7|     6|    11|   24|           11|\n|  12|              Brazil|   7|     6|     8|   21|           12|\n|  13|         New Zealand|   7|     6|     7|   20|           13|\n|  14|                Cuba|   7|     3|     5|   15|           18|\n|  15|             Hungary|   6|     7|     7|   20|           13|\n|  16|   Republic of Korea|   6|     4|    10|   20|           13|\n|  17|              Poland|   4|     5|     5|   14|           19|\n|  18|      Czech Republic|   4|     4|     3|   11|           23|\n|  19|               Kenya|   4|     4|     2|   10|           25|\n|  20|              Norway|   4|     2|     2|    8|           29|\n+----+--------------------+----+------+------+-----+-------------+\nonly showing top 20 rows\n\nroot\n |-- Rank: integer (nullable = true)\n |-- TeamCountry: string (nullable = true)\n |-- Gold: integer (nullable = true)\n |-- Silver: integer (nullable = true)\n |-- Bronze: integer (nullable = true)\n |-- Total: integer (nullable = true)\n |-- Rank by Total: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "Medalsdf.show()\n",
    "Medalsdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a95591-7f3c-4f57-bb7f-757bef1c7c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------------+------------+\n|     TeamName|    Discipline|             Country|       Event|\n+-------------+--------------+--------------------+------------+\n|      Belgium|3x3 Basketball|             Belgium|         Men|\n|        China|3x3 Basketball|People's Republic...|         Men|\n|        China|3x3 Basketball|People's Republic...|       Women|\n|       France|3x3 Basketball|              France|       Women|\n|        Italy|3x3 Basketball|               Italy|       Women|\n|        Japan|3x3 Basketball|               Japan|         Men|\n|        Japan|3x3 Basketball|               Japan|       Women|\n|       Latvia|3x3 Basketball|              Latvia|         Men|\n|     Mongolia|3x3 Basketball|            Mongolia|       Women|\n|  Netherlands|3x3 Basketball|         Netherlands|         Men|\n|       Poland|3x3 Basketball|              Poland|         Men|\n|          ROC|3x3 Basketball|                 ROC|         Men|\n|          ROC|3x3 Basketball|                 ROC|       Women|\n|      Romania|3x3 Basketball|             Romania|       Women|\n|       Serbia|3x3 Basketball|              Serbia|         Men|\n|United States|3x3 Basketball|United States of ...|       Women|\n|    Australia|       Archery|           Australia|  Men's Team|\n|    Australia|       Archery|           Australia|  Mixed Team|\n|   Bangladesh|       Archery|          Bangladesh|  Mixed Team|\n|      Belarus|       Archery|             Belarus|Women's Team|\n+-------------+--------------+--------------------+------------+\nonly showing top 20 rows\n\nroot\n |-- TeamName: string (nullable = true)\n |-- Discipline: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- Event: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "Teamsdf.show()\n",
    "Teamsdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84558226-1bc4-498b-b273-714e51938579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+-----------+\n|         Teamcountry|gold|sum(Silver)|sum(Bronze)|\n+--------------------+----+-----------+-----------+\n|United States of ...|  39|         41|         33|\n|People's Republic...|  38|         32|         18|\n|               Japan|  27|         14|         17|\n|       Great Britain|  22|         21|         22|\n|                 ROC|  20|         28|         23|\n|           Australia|  17|          7|         22|\n|             Germany|  10|         11|         16|\n|              France|  10|         12|         11|\n|               Italy|  10|         10|         20|\n|         Netherlands|  10|         12|         14|\n|                Cuba|   7|          3|          5|\n|              Canada|   7|          6|         11|\n|              Brazil|   7|          6|          8|\n|         New Zealand|   7|          6|          7|\n|   Republic of Korea|   6|          4|         10|\n|             Hungary|   6|          7|          7|\n|              Norway|   4|          2|          2|\n|             Jamaica|   4|          1|          4|\n|      Czech Republic|   4|          4|          3|\n|               Kenya|   4|          4|          2|\n|              Poland|   4|          5|          5|\n|Islamic Republic ...|   3|          2|          2|\n|              Sweden|   3|          6|          0|\n|             Belgium|   3|          1|          3|\n|             Croatia|   3|          3|          2|\n|               Spain|   3|          8|          6|\n|             Denmark|   3|          4|          4|\n|         Switzerland|   3|          4|          6|\n|          Uzbekistan|   3|          0|          2|\n|            Slovenia|   3|          1|          1|\n|            Bulgaria|   3|          1|          2|\n|              Serbia|   3|          1|          5|\n|      Chinese Taipei|   2|          4|          6|\n|              Turkey|   2|          2|          9|\n|              Greece|   2|          1|          1|\n|              Kosovo|   2|          0|          0|\n|             Ecuador|   2|          1|          0|\n|               Qatar|   2|          0|          1|\n|             Bahamas|   2|          0|          0|\n|             Ireland|   2|          0|          2|\n|              Israel|   2|          0|          2|\n|             Georgia|   2|          5|          1|\n|              Uganda|   2|          1|          1|\n|         Philippines|   1|          2|          1|\n|                Fiji|   1|          0|          1|\n|            Slovakia|   1|          2|          1|\n|    Hong Kong, China|   1|          2|          3|\n|               India|   1|          2|          4|\n|             Belarus|   1|          3|          3|\n|         Puerto Rico|   1|          0|          0|\n|            Thailand|   1|          0|          1|\n|             Morocco|   1|          0|          0|\n|             Ukraine|   1|          6|         12|\n|           Venezuela|   1|          3|          0|\n|             Estonia|   1|          0|          1|\n|           Indonesia|   1|          1|          3|\n|             Tunisia|   1|          1|          0|\n|            Ethiopia|   1|          1|          2|\n|              Latvia|   1|          0|          1|\n|            Portugal|   1|          1|          2|\n|             Romania|   1|          3|          0|\n|             Austria|   1|          1|          5|\n|               Egypt|   1|          1|          4|\n|        South Africa|   1|          2|          0|\n|             Bermuda|   1|          0|          0|\n|       CÃ´te d'Ivoire|   0|          0|          1|\n|            Malaysia|   0|          1|          1|\n|              Jordan|   0|          1|          1|\n|           Argentina|   0|          1|          2|\n|          San Marino|   0|          1|          2|\n|             Finland|   0|          0|          2|\n|               Ghana|   0|          0|          1|\n|              Kuwait|   0|          0|          1|\n|             Nigeria|   0|          1|          1|\n|           Lithuania|   0|          1|          0|\n|        Turkmenistan|   0|          1|          0|\n|              Mexico|   0|          0|          4|\n|            Mongolia|   0|          1|          3|\n|          Azerbaijan|   0|          3|          4|\n|             Grenada|   0|          0|          1|\n|             Armenia|   0|          2|          2|\n|        Saudi Arabia|   0|          1|          0|\n|             Namibia|   0|          1|          0|\n|          Kyrgyzstan|   0|          2|          1|\n|     North Macedonia|   0|          1|          0|\n|  Dominican Republic|   0|          3|          2|\n|            Botswana|   0|          0|          1|\n| Republic of Moldova|   0|          0|          1|\n|          Kazakhstan|   0|          0|          8|\n|        Burkina Faso|   0|          0|          1|\n|             Bahrain|   0|          1|          0|\n|            Colombia|   0|          4|          1|\n|Syrian Arab Republic|   0|          0|          1|\n+--------------------+----+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Total medals won by each country \n",
    "Medalsdf.createOrReplaceTempView('Testsql')\n",
    "a=spark.sql('select Teamcountry,sum(Gold) gold,sum(Silver),sum(Bronze) from Testsql group by Teamcountry order by gold desc')\n",
    "a.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ea0cb4-ecb0-4f87-9276-439b4efdc0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1429101508094252>:5\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m Medalsdf_rename \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank by Total\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank_by_Total\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 5\u001B[0m Medalsdf_rename\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals_rename.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m Medals_renamed \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals_rename.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/Medals_rename.csv already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1429101508094252>:5\u001B[0m\n\u001B[1;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m Medalsdf_rename \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank by Total\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRank_by_Total\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m Medalsdf_rename\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals_rename.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m Medals_renamed \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/FileStore/tables/Medals_rename.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/Medals_rename.csv already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/tables/Medals_rename.csv already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Rename Medal file \n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/FileStore/tables/Medals.csv\")\n",
    "Medalsdf_rename = df.withColumnRenamed(\"Rank by Total\",\"Rank_by_Total\")\n",
    "Medalsdf_rename.write.option(\"header\", \"true\").csv(\"/FileStore/tables/Medals_rename.csv\")\n",
    "Medals_renamed = spark.read.option(\"header\", \"true\").csv(\"/FileStore/tables/Medals_rename.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2efbe0d3-7bbc-4273-b2cd-78f67436feed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Medallion architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c81b667-5842-4049-9b78-5525e4d83124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7820a435-17f6-442e-87eb-d9a31c08ce5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1: Athletes Data\n",
    "df_raw_athletes = spark.read.csv(\"/FileStore/tables/Athletes.csv\", header=True, inferSchema=True)\n",
    "df_raw_athletes.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/bronze_data/athletes\")\n",
    "\n",
    "# 2: Coaches Data\n",
    "df_raw_coaches = spark.read.csv(\"/FileStore/tables/Coaches.csv\", header=True, inferSchema=True)\n",
    "df_raw_coaches.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/bronze_data/coaches\")\n",
    "\n",
    "# 3: EntriesGender Data\n",
    "df_raw_entries_gender = spark.read.csv(\"/FileStore/tables/EntriesGender.csv\", header=True, inferSchema=True)\n",
    "df_raw_entries_gender.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/bronze_data/entries_gender\")\n",
    "\n",
    "# 4: Medals Data\n",
    "df_raw_medals = spark.read.csv(\"/FileStore/tables/Medals_rename.csv\", header=True, inferSchema=True)\n",
    "df_raw_medals.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/bronze_data/medals\")\n",
    "\n",
    "# 5: Teams Data\n",
    "df_raw_teams = spark.read.csv(\"/FileStore/tables/Teams.csv\", header=True, inferSchema=True)\n",
    "df_raw_teams.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/tables/bronze_data/teams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdb65c5-043b-4ac7-91c2-45d63dac08f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [FileInfo(path='dbfs:/FileStore/tables/bronze_data/athletes/', name='athletes/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/bronze_data/coaches/', name='coaches/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/bronze_data/entries_gender/', name='entries_gender/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/bronze_data/medals/', name='medals/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/bronze_data/teams/', name='teams/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/bronze_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d521330-0faf-4322-b769-d63fac069fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d878539-42ab-472a-84ad-1628d6b8453e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Rank: integer (nullable = true)\n |-- TeamCountry: string (nullable = true)\n |-- Gold: integer (nullable = true)\n |-- Silver: integer (nullable = true)\n |-- Bronze: integer (nullable = true)\n |-- Total: integer (nullable = true)\n |-- Rank_by_Total: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "clean_path = \"/FileStore/tables/silver_data\"\n",
    "df_clean_medals = df_raw_medals.withColumn(\"Total\", col(\"Total\").cast(\"int\"))\n",
    "df_clean.write.format(\"delta\").mode(\"overwrite\").save(clean_path) \n",
    "df_clean_medals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d016daa-623c-4b74-9785-bbae3c2b6147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1128460015626671>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m df_raw_teams\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteamreplace\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 2\u001B[0m df_clean_teams\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect rank,TeamCountry, gold,silver,bronze,total, cast(rank_by_toal as int)  from teamreplace\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m df_clean_teams\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `rank` cannot be resolved. Did you mean one of the following? [`teamreplace`.`Event`, `teamreplace`.`Country`, `teamreplace`.`TeamName`, `teamreplace`.`Discipline`].; line 1 pos 7;\n",
       "'Project ['rank, 'TeamCountry, 'gold, 'silver, 'bronze, 'total, unresolvedalias(cast('rank_by_toal as int), None)]\n",
       "+- SubqueryAlias teamreplace\n",
       "   +- View (`teamreplace`, [TeamName#3993,Discipline#3994,Country#3995,Event#3996])\n",
       "      +- Relation [TeamName#3993,Discipline#3994,Country#3995,Event#3996] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1128460015626671>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m df_raw_teams\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mteamreplace\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m df_clean_teams\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect rank,TeamCountry, gold,silver,bronze,total, cast(rank_by_toal as int)  from teamreplace\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m df_clean_teams\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `rank` cannot be resolved. Did you mean one of the following? [`teamreplace`.`Event`, `teamreplace`.`Country`, `teamreplace`.`TeamName`, `teamreplace`.`Discipline`].; line 1 pos 7;\n'Project ['rank, 'TeamCountry, 'gold, 'silver, 'bronze, 'total, unresolvedalias(cast('rank_by_toal as int), None)]\n+- SubqueryAlias teamreplace\n   +- View (`teamreplace`, [TeamName#3993,Discipline#3994,Country#3995,Event#3996])\n      +- Relation [TeamName#3993,Discipline#3994,Country#3995,Event#3996] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `rank` cannot be resolved. Did you mean one of the following? [`teamreplace`.`Event`, `teamreplace`.`Country`, `teamreplace`.`TeamName`, `teamreplace`.`Discipline`].; line 1 pos 7;\n'Project ['rank, 'TeamCountry, 'gold, 'silver, 'bronze, 'total, unresolvedalias(cast('rank_by_toal as int), None)]\n+- SubqueryAlias teamreplace\n   +- View (`teamreplace`, [TeamName#3993,Discipline#3994,Country#3995,Event#3996])\n      +- Relation [TeamName#3993,Discipline#3994,Country#3995,Event#3996] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw_teams.createOrReplaceTempView('teamreplace')\n",
    "df_clean_teams=spark.sql(\"select rank,TeamCountry, gold,silver,bronze,total, cast(rank_by_toal as int)  from teamreplace\")\n",
    "df_clean_teams.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2fb159b-4746-4798-8745-0ae629a07158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----+------+------+-----+-------------+\n|Rank|         TeamCountry|Gold|Silver|Bronze|Total|Rank_by_Total|\n+----+--------------------+----+------+------+-----+-------------+\n|   1|United States of ...|  39|    41|    33|  113|            1|\n|   2|People's Republic...|  38|    32|    18|   88|            2|\n|   3|               Japan|  27|    14|    17|   58|            5|\n|   4|       Great Britain|  22|    21|    22|   65|            4|\n|   5|                 ROC|  20|    28|    23|   71|            3|\n|   6|           Australia|  17|     7|    22|   46|            6|\n|   7|         Netherlands|  10|    12|    14|   36|            9|\n|   8|              France|  10|    12|    11|   33|           10|\n|   9|             Germany|  10|    11|    16|   37|            8|\n|  10|               Italy|  10|    10|    20|   40|            7|\n|  11|              Canada|   7|     6|    11|   24|           11|\n|  12|              Brazil|   7|     6|     8|   21|           12|\n|  13|         New Zealand|   7|     6|     7|   20|           13|\n|  14|                Cuba|   7|     3|     5|   15|           18|\n|  15|             Hungary|   6|     7|     7|   20|           13|\n|  16|   Republic of Korea|   6|     4|    10|   20|           13|\n|  17|              Poland|   4|     5|     5|   14|           19|\n|  18|      Czech Republic|   4|     4|     3|   11|           23|\n|  19|               Kenya|   4|     4|     2|   10|           25|\n|  20|              Norway|   4|     2|     2|    8|           29|\n+----+--------------------+----+------+------+-----+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#df_raw_athletes.show()\n",
    "df_raw_medals.show()\n",
    "#df_raw_coaches.show()\n",
    "#df_raw_entries_gender.show()\n",
    "#df_raw_teams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6fc1c68-0deb-4d5d-bb77-5039bd4d00c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----+------+------+-----+-------------+\n|Rank|         TeamCountry|Gold|Silver|Bronze|Total|Rank_by_Total|\n+----+--------------------+----+------+------+-----+-------------+\n|   1|United States of ...|  39|    41|    33|  113|            1|\n|   2|People's Republic...|  38|    32|    18|   88|            2|\n|   3|               Japan|  27|    14|    17|   58|            5|\n|   4|       Great Britain|  22|    21|    22|   65|            4|\n|   5|                 ROC|  20|    28|    23|   71|            3|\n|   6|           Australia|  17|     7|    22|   46|            6|\n|   7|         Netherlands|  10|    12|    14|   36|            9|\n|   8|              France|  10|    12|    11|   33|           10|\n|   9|             Germany|  10|    11|    16|   37|            8|\n|  10|               Italy|  10|    10|    20|   40|            7|\n|  11|              Canada|   7|     6|    11|   24|           11|\n|  12|              Brazil|   7|     6|     8|   21|           12|\n|  13|         New Zealand|   7|     6|     7|   20|           13|\n|  14|                Cuba|   7|     3|     5|   15|           18|\n|  15|             Hungary|   6|     7|     7|   20|           13|\n|  16|   Republic of Korea|   6|     4|    10|   20|           13|\n|  17|              Poland|   4|     5|     5|   14|           19|\n|  18|      Czech Republic|   4|     4|     3|   11|           23|\n|  19|               Kenya|   4|     4|     2|   10|           25|\n|  20|              Norway|   4|     2|     2|    8|           29|\n+----+--------------------+----+------+------+-----+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#df_nulls = df_raw_coaches.filter(col(\"Event\").isNull())\n",
    "#df_nulls.show(100)\n",
    "df_raw_medals.createOrReplaceTempView('medalreplace')\n",
    "replace_value=spark.sql(\"select * from medalreplace   \")\n",
    "replace_value.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f336a2-5bc5-4d43-b0b3-f24e5ec0dc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "OlympicDatasetTransformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
